{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Dict\n",
    "\n",
    "import lightning as L\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import torch.nn.functional as F\n",
    "from lightning import LightningModule, LightningDataModule\n",
    "import hydra\n",
    "from omegaconf import DictConfig\n",
    "from PIL import Image\n",
    "import json\n",
    "import dlearn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "from lightning import LightningModule\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "from einops import rearrange, reduce, repeat\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from torchmetrics import MaxMetric, MeanMetric\n",
    "from torchmetrics.classification.accuracy import Accuracy\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels=3,\n",
    "        patch_size=16,\n",
    "        emb_size=64,\n",
    "        img_size=224,\n",
    "    ):\n",
    "        super(PatchEmbedding, self).__init__()\n",
    "\n",
    "        assert (\n",
    "            img_size / patch_size % 1 == 0\n",
    "        ), \"img_size must be integer multiple of patch_size\"\n",
    "\n",
    "        self.projection = nn.Sequential(\n",
    "            Rearrange(\n",
    "                \"b c (h s1) (w s2) -> b (h w) (s1 s2 c)\", s1=patch_size, s2=patch_size\n",
    "            ),\n",
    "            nn.Linear(patch_size * patch_size * in_channels, emb_size),\n",
    "        )\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, emb_size))\n",
    "\n",
    "        self.positional_emb = nn.Parameter(\n",
    "            torch.randn(\n",
    "                (img_size // patch_size) * (img_size // patch_size)\n",
    "                + 1,  # 14 x 14 patches + CLS patch\n",
    "                emb_size,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, *_ = x.shape\n",
    "        x = self.projection(x)\n",
    "        # print(x.shape, )\n",
    "        cls_token = repeat(self.cls_token, \"() p e -> b p e\", b=B)\n",
    "\n",
    "        # print(cls_token.shape)\n",
    "\n",
    "        x = torch.cat([cls_token, x], dim=1)\n",
    "\n",
    "        x += self.positional_emb\n",
    "\n",
    "        return x\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, emb_size=768, num_heads=8, dropout=0):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.emb_size = emb_size\n",
    "\n",
    "        self.query = nn.Linear(emb_size, emb_size)\n",
    "        self.key = nn.Linear(emb_size, emb_size)\n",
    "        self.value = nn.Linear(emb_size, emb_size)\n",
    "\n",
    "        self.projection = nn.Linear(emb_size, emb_size)\n",
    "\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.scaling = (self.emb_size // num_heads) ** -0.5\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        rearrange_heads = (\n",
    "            \"batch seq_len (num_head h_dim) -> batch num_head seq_len h_dim\"\n",
    "        )\n",
    "\n",
    "        queries = rearrange(self.query(x), rearrange_heads, num_head=self.num_heads)\n",
    "        keys = rearrange(self.key(x), rearrange_heads, num_head=self.num_heads)\n",
    "\n",
    "        values = rearrange(self.key(x), rearrange_heads, num_head=self.num_heads)\n",
    "\n",
    "        energies = torch.einsum(\"bhqd, bhkd -> bhqk\", queries, keys)\n",
    "\n",
    "        if mask is not None:\n",
    "            fill_value = torch.finfo(energies.dtype).min\n",
    "            energies.mask_fill(~mask, fill_value)\n",
    "\n",
    "        attention = F.softmax(energies, dim=-1) * self.scaling\n",
    "\n",
    "        attention = self.attn_dropout(attention)\n",
    "\n",
    "        out = torch.einsum(\"bhas, bhsd -> bhad\", attention, values)\n",
    "\n",
    "        out = rearrange(\n",
    "            out, \"batch num_head seq_length dim -> batch seq_length (num_head dim)\"\n",
    "        )\n",
    "\n",
    "        out = self.projection(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class ResidualAdd(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super(ResidualAdd, self).__init__()\n",
    "\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        res = x\n",
    "\n",
    "        out = self.fn(x, **kwargs)\n",
    "\n",
    "        out += res\n",
    "\n",
    "        return out\n",
    "\n",
    "FeedForwardBlock = lambda emb_size=768, expansion=4, drop_p=0.0: nn.Sequential(\n",
    "    nn.Linear(emb_size, expansion * emb_size),\n",
    "    nn.GELU(),\n",
    "    nn.Dropout(drop_p),\n",
    "    nn.Linear(expansion * emb_size, emb_size),\n",
    ")\n",
    "\n",
    "class TransformerEncoderBlock(nn.Sequential):\n",
    "    def __init__(\n",
    "        self, emb_size=768, drop_p=0.0, forward_expansion=4, forward_drop_p=0, **kwargs\n",
    "    ):\n",
    "        super(TransformerEncoderBlock, self).__init__(\n",
    "            ResidualAdd(\n",
    "                nn.Sequential(\n",
    "                    nn.LayerNorm(emb_size),\n",
    "                    MultiHeadAttention(emb_size, **kwargs),\n",
    "                    nn.Dropout(drop_p),\n",
    "                )\n",
    "            ),\n",
    "            ResidualAdd(\n",
    "                nn.Sequential(\n",
    "                    nn.LayerNorm(emb_size),\n",
    "                    FeedForwardBlock(\n",
    "                        emb_size, expansion=forward_expansion, drop_p=forward_drop_p\n",
    "                    ),\n",
    "                    nn.Dropout(drop_p),\n",
    "                )\n",
    "            ),\n",
    "        )\n",
    "\n",
    "class TransformerEncoder(nn.Sequential):\n",
    "    def __init__(self, depth=12, **kwargs):\n",
    "        super(TransformerEncoder, self).__init__(\n",
    "            *(TransformerEncoderBlock(**kwargs) for _ in range(depth))\n",
    "        )\n",
    "\n",
    "class ClassificationHead(nn.Sequential):\n",
    "    def __init__(self, emb_size=768, num_classes=1000):\n",
    "        super(ClassificationHead, self).__init__(\n",
    "            Reduce(\n",
    "                \"batch_size seq_len emb_dim -> batch_size emb_dim\", reduction=\"mean\"\n",
    "            ),\n",
    "            nn.LayerNorm(emb_size),\n",
    "            nn.Linear(emb_size, num_classes),\n",
    "        )\n",
    "\n",
    "class ViT(nn.Sequential):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels=3,\n",
    "        patch_size=16,\n",
    "        emb_size=768,\n",
    "        img_size=224,\n",
    "        depth=12,\n",
    "        num_classes=1000,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super(ViT, self).__init__(\n",
    "            PatchEmbedding(\n",
    "                in_channels,\n",
    "                patch_size,\n",
    "                emb_size,\n",
    "                img_size,\n",
    "            ),\n",
    "            TransformerEncoder(depth, emb_size=emb_size, **kwargs),\n",
    "            ClassificationHead(emb_size, num_classes),\n",
    "        )\n",
    "\n",
    "class VitLitModule(LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        optimizer: torch.optim.Optimizer,\n",
    "        scheduler: torch.optim.lr_scheduler,\n",
    "        num_classes=2,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters(logger=False, ignore=[\"model\"])\n",
    "\n",
    "        self.model = ViT(\n",
    "            in_channels=3,\n",
    "            patch_size=4,\n",
    "            emb_size=64,\n",
    "            img_size=32,\n",
    "            depth=6,\n",
    "            num_classes=num_classes,\n",
    "        )\n",
    "\n",
    "        # loss function\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        # metric objects for calculating and averaging accuracy across batches\n",
    "        self.train_acc = Accuracy(\n",
    "            task=\"multiclass\", num_classes=self.hparams.num_classes\n",
    "        )\n",
    "        self.val_acc = Accuracy(task=\"multiclass\", num_classes=self.hparams.num_classes)\n",
    "        self.test_acc = Accuracy(\n",
    "            task=\"multiclass\", num_classes=self.hparams.num_classes\n",
    "        )\n",
    "\n",
    "        # for averaging loss across batches\n",
    "        self.train_loss = MeanMetric()\n",
    "        self.val_loss = MeanMetric()\n",
    "        self.test_loss = MeanMetric()\n",
    "\n",
    "        # for tracking best so far validation accuracy\n",
    "        self.val_acc_best = MaxMetric()\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.model(x)\n",
    "\n",
    "    def model_step(self, batch: Any):\n",
    "        x, y = batch\n",
    "        logits = self.forward(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        return loss, preds, y\n",
    "\n",
    "    def training_step(self, batch: Any, batch_idx: int):\n",
    "        loss, preds, targets = self.model_step(batch)\n",
    "\n",
    "        # update and log metrics\n",
    "        self.train_loss(loss)\n",
    "        self.train_acc(preds, targets)\n",
    "        self.log(\n",
    "            \"train/loss\", self.train_loss, on_step=False, on_epoch=True, prog_bar=True\n",
    "        )\n",
    "        self.log(\n",
    "            \"train/acc\", self.train_acc, on_step=False, on_epoch=True, prog_bar=True\n",
    "        )\n",
    "\n",
    "        # return loss or backpropagation will fail\n",
    "        return loss\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        pass\n",
    "\n",
    "    def validation_step(self, batch: Any, batch_idx: int):\n",
    "        loss, preds, targets = self.model_step(batch)\n",
    "\n",
    "        # update and log metrics\n",
    "        self.val_loss(loss)\n",
    "        self.val_acc(preds, targets)\n",
    "        self.log(\"val/loss\", self.val_loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"val/acc\", self.val_acc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        acc = self.val_acc.compute()  # get current val acc\n",
    "        self.val_acc_best(acc)  # update best so far val acc\n",
    "        # log `val_acc_best` as a value through `.compute()` method, instead of as a metric object\n",
    "        # otherwise metric would be reset by lightning after each epoch\n",
    "        self.log(\n",
    "            \"val/acc_best\", self.val_acc_best.compute(), sync_dist=True, prog_bar=True\n",
    "        )\n",
    "\n",
    "    def test_step(self, batch: Any, batch_idx: int):\n",
    "        loss, preds, targets = self.model_step(batch)\n",
    "\n",
    "        # update and log metrics\n",
    "        self.test_loss(loss)\n",
    "        self.test_acc(preds, targets)\n",
    "        self.log(\n",
    "            \"test/loss\", self.test_loss, on_step=False, on_epoch=True, prog_bar=True\n",
    "        )\n",
    "        self.log(\"test/acc\", self.test_acc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        pass\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"Choose what optimizers and learning-rate schedulers to use in your optimization.\n",
    "        Normally you'd need one. But in the case of GANs or similar you might have multiple.\n",
    "\n",
    "        Examples:\n",
    "            <https://lightning.ai/docs/pytorch/latest/common/lightning_module.html#configure-optimizers>\n",
    "        \"\"\"\n",
    "        optimizer = self.hparams.optimizer(params=self.parameters())\n",
    "        if self.hparams.scheduler is not None:\n",
    "            scheduler = self.hparams.scheduler(optimizer=optimizer)\n",
    "            return {\n",
    "                \"optimizer\": optimizer,\n",
    "                \"lr_scheduler\": {\n",
    "                    \"scheduler\": scheduler,\n",
    "                    \"monitor\": \"val/loss\",\n",
    "                    \"interval\": \"epoch\",\n",
    "                    \"frequency\": 1,\n",
    "                },\n",
    "            }\n",
    "        return {\"optimizer\": optimizer}\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ViT(in_channels=3,\n",
    "            patch_size=4,\n",
    "            emb_size=64,\n",
    "            img_size=32,\n",
    "            depth=6,\n",
    "            num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "module = VitLitModule(optimizer=torch.optim.Adam, scheduler=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = module.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2+3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt_path = '/home/ubuntu/workefs/E3_S5/outputs//2023-06-18/08-42-02/checkpoints/best.ckpt'\n",
    "loaded_ckpt = torch.load(ckpt_path, map_location=torch.device('cpu'))\n",
    "my_model_kvpair=model.state_dict()\n",
    "for key,value in loaded_ckpt['state_dict'].items():\n",
    "    my_key = key[6:]\n",
    "    my_model_kvpair[my_key] = loaded_ckpt['state_dict'][key]\n",
    "model.load_state_dict(my_model_kvpair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
